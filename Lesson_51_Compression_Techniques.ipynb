{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_pruning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hkUaPAmJj-Wg",
        "outputId": "a1f75dc4-7e28-486d-ceeb-3ca38ac2a026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_pruning\n",
            "  Downloading torch_pruning-1.6.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torch_pruning) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_pruning) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torch_pruning) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->torch_pruning) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torch_pruning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torch_pruning) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torch_pruning) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->torch_pruning)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->torch_pruning)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->torch_pruning)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->torch_pruning)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->torch_pruning)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->torch_pruning)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->torch_pruning)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->torch_pruning)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->torch_pruning)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->torch_pruning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torch_pruning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torch_pruning) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->torch_pruning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->torch_pruning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torch_pruning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torch_pruning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torch_pruning) (3.0.2)\n",
            "Downloading torch_pruning-1.6.0-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch_pruning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch_pruning-1.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Метод                                    | Библиотека                                              | Комментарий                                                                                     |\n",
        "|------------------------------------------|---------------------------------------------------------|------------------------------------------------------------------------------------------------|\n",
        "| Прунинг                                 | TorchPruning                                            | Лёгкая, гибкая. Работает прямо с PyTorch. Поддерживает структурный и неструктурный pruning.     |\n",
        "| Квантизация                            | Optimum + ONNX/OpenVINO/Intel Neural Compressor        | Поддержка quantization-aware training (QAT) и post-training quantization (PTQ). Имеет интеграцию с HuggingFace. |\n",
        "| Дистилляция                            | HuggingFace Transformers + DistillationTrainer        | Есть готовый код в примерах. Работает прямо с Trainer API.                                    |\n",
        "| Низкоранговая факторизация (SVD/LoRA) | peft                                                    | Поддержка LoRA/IA3, особенно хороша для больших моделей. Для SVD есть torch.nn.utils.prune или кастомные имплементации. |\n",
        "| Все-в-одном (bonus)                     | Neural Compressor, SparseML, NNCF                     | Поддерживают pruning + quantization + distillation. Более комплексные. Лучше подходят под продакшн. |"
      ],
      "metadata": {
        "id": "4T0-vgdX7cCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Подготовка"
      ],
      "metadata": {
        "id": "s59eFV-A0t-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch_pruning as tp\n",
        "import time\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_ckpt = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=2).to(device)\n",
        "\n",
        "raw_dataset = load_dataset(\"glue\", \"sst2\")\n",
        "\n",
        "def preprocess(example):\n",
        "    return tokenizer(example[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "encoded_dataset = raw_dataset.map(preprocess, batched=True)\n",
        "encoded_dataset = encoded_dataset.remove_columns([\"sentence\", \"idx\"])\n",
        "encoded_dataset.set_format(\"torch\")\n",
        "\n",
        "train_dataset = encoded_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "val_dataset = encoded_dataset[\"validation\"].select(range(200))\n",
        "\n",
        "collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "def custom_collate(batch):\n",
        "    features = [{k: v for k, v in example.items() if k != \"label\"} for example in batch]\n",
        "    labels = torch.tensor([example[\"label\"] for example in batch])\n",
        "    padded = collator(features)\n",
        "    padded[\"label\"] = labels\n",
        "    return padded\n",
        "\n",
        "def evaluate(model, dataset, batch_size=16):\n",
        "    model.eval()\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=custom_collate)\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            labels = batch.pop(\"label\").to(device)\n",
        "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**inputs)\n",
        "            preds = outputs.logits.argmax(dim=-1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += len(labels)\n",
        "    return correct / total\n",
        "\n",
        "def measure_size(model):\n",
        "    torch.save(model.state_dict(), \"temp.pt\")\n",
        "    size_mb = os.path.getsize(\"temp.pt\") / 1e6\n",
        "    os.remove(\"temp.pt\")\n",
        "    return size_mb\n",
        "\n",
        "def measure_inference_time(model, dataset, batch_size=16, runs=5):\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=custom_collate)\n",
        "    model.eval()\n",
        "    times = []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            if i >= runs: break\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != \"label\"}\n",
        "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
        "            start = time.time()\n",
        "            model(**inputs)\n",
        "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
        "            times.append(time.time() - start)\n",
        "    return sum(times) / len(times)\n",
        "\n",
        "baseline_acc = evaluate(model, val_dataset)\n",
        "baseline_size = measure_size(model)\n",
        "baseline_time = measure_inference_time(model, val_dataset)\n",
        "\n",
        "print(f\"📊 Baseline accuracy: {baseline_acc:.4f}\")\n",
        "print(f\"📦 Baseline size: {baseline_size:.2f} MB\")\n",
        "print(f\"⚡️ Baseline inference time: {baseline_time:.4f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyvhvh0fqE5Z",
        "outputId": "e82fc4aa-7145-4f4a-ad4f-d54201753190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Baseline accuracy: 0.4950\n",
            "📦 Baseline size: 267.85 MB\n",
            "⚡️ Baseline inference time: 0.0650 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Pruning (torch_pruning)"
      ],
      "metadata": {
        "id": "inDWhS1W0xaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "example_inputs = {\n",
        "    \"input_ids\": torch.ones(1, 128, dtype=torch.long).to(device),\n",
        "    \"attention_mask\": torch.ones(1, 128, dtype=torch.long).to(device)\n",
        "}\n",
        "\n",
        "class PrunableWrapper(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "\n",
        "wrapped_model = PrunableWrapper(model)\n",
        "\n",
        "ignored_layers = []\n",
        "for name, m in model.named_modules():\n",
        "    if isinstance(m, nn.Linear):\n",
        "        if \"classifier\" in name or \"attention\" in name:\n",
        "            ignored_layers.append(m)\n",
        "\n",
        "imp = tp.importance.MagnitudeImportance(p=2, group_reduction='mean')\n",
        "\n",
        "pruner = tp.pruner.MagnitudePruner(\n",
        "    wrapped_model,\n",
        "    example_inputs=(example_inputs[\"input_ids\"], example_inputs[\"attention_mask\"]),\n",
        "    importance=imp,\n",
        "    global_pruning=False,\n",
        "    pruning_ratio=0.5,\n",
        "    iterative_steps=2,\n",
        "    ignored_layers=ignored_layers,\n",
        ")\n",
        "\n",
        "for i in range(2):\n",
        "    pruner.step()\n",
        "    macs, params = tp.utils.count_ops_and_params(wrapped_model, (example_inputs[\"input_ids\"], example_inputs[\"attention_mask\"]))\n",
        "    print(f\"🧹 Iter {i+1}: Params {params / 1e6:.2f}M, MACs {macs / 1e9:.2f}G\")\n",
        "\n",
        "print(\"✂️ Pruning complete.\")\n",
        "\n",
        "model.train()\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate)\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
        "\n",
        "EPOCHS = 3\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        labels = batch.pop(\"label\").to(device)\n",
        "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"📚 Epoch {epoch+1} — Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "pruned_acc = evaluate(model, val_dataset)\n",
        "pruned_size = measure_size(model)\n",
        "pruned_time = measure_inference_time(model, val_dataset)\n",
        "macs, params = tp.utils.count_ops_and_params(wrapped_model, (example_inputs[\"input_ids\"], example_inputs[\"attention_mask\"]))\n",
        "\n",
        "model.save_pretrained(\"finetuned_pruned_model\")\n",
        "tokenizer.save_pretrained(\"finetuned_pruned_model\")\n",
        "\n",
        "print(\"\\n✅ After Pruning & Fine-tuning:\")\n",
        "print(f\"📊 Accuracy: {pruned_acc:.4f}\")\n",
        "print(f\"📦 Size: {pruned_size:.2f} MB\")\n",
        "print(f\"⚡️ Inference time: {pruned_time:.4f} sec\")\n",
        "print(f\"🔢 MACs: {macs / 1e9:.2f} G, Params: {params / 1e6:.2f} M\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp2Op2LkqFZg",
        "outputId": "6366e6db-0392-4273-8344-7e748d0cc96f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 Iter 1: Params 59.87M, MACs 4.53G\n",
            "🧹 Iter 2: Params 52.79M, MACs 3.63G\n",
            "✂️ Pruning complete.\n",
            "📚 Epoch 1 — Loss: 0.6441\n",
            "📚 Epoch 2 — Loss: 0.4227\n",
            "📚 Epoch 3 — Loss: 0.2114\n",
            "\n",
            "✅ After Pruning & Fine-tuning:\n",
            "📊 Accuracy: 0.7400\n",
            "📦 Size: 211.19 MB\n",
            "⚡️ Inference time: 0.0332 sec\n",
            "🔢 MACs: 3.63 G, Params: 52.79 M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch_pruning as tp\n",
        "import gc, os, time\n",
        "from transformers import AutoModelForSequenceClassification, get_scheduler\n",
        "\n",
        "def measure_size(model):\n",
        "    torch.save(model.state_dict(), \"/tmp/temp_model.pth\")\n",
        "    return os.path.getsize(\"/tmp/temp_model.pth\") / 1e6\n",
        "\n",
        "def measure_inference_time(model, dataset, batch_size=16):\n",
        "    model.eval()\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=custom_collate)\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != \"label\"}\n",
        "            model(**inputs)\n",
        "    end = time.time()\n",
        "    return (end - start) / len(dataloader)\n",
        "\n",
        "# Конфигурация эксперимента\n",
        "ratios_to_try = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "results = []\n",
        "\n",
        "for ratio in ratios_to_try:\n",
        "    print(f\"\\n🔧 Testing pruning_ratio = {ratio}\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=2).to(device)\n",
        "    model.train()\n",
        "\n",
        "    class PrunableWrapper(nn.Module):\n",
        "        def __init__(self, model):\n",
        "            super().__init__()\n",
        "            self.model = model\n",
        "\n",
        "        def forward(self, input_ids, attention_mask):\n",
        "            return self.model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "\n",
        "    wrapped_model = PrunableWrapper(model)\n",
        "\n",
        "    ignored_layers = []\n",
        "    for name, m in model.named_modules():\n",
        "        if isinstance(m, nn.Linear) and (\"classifier\" in name or \"attention\" in name):\n",
        "            ignored_layers.append(m)\n",
        "\n",
        "    example_inputs = {\n",
        "        \"input_ids\": torch.ones(1, 128, dtype=torch.long).to(device),\n",
        "        \"attention_mask\": torch.ones(1, 128, dtype=torch.long).to(device)\n",
        "    }\n",
        "\n",
        "    imp = tp.importance.MagnitudeImportance(p=2, group_reduction='mean')\n",
        "    pruner = tp.pruner.MagnitudePruner(\n",
        "        wrapped_model,\n",
        "        example_inputs=(example_inputs[\"input_ids\"], example_inputs[\"attention_mask\"]),\n",
        "        importance=imp,\n",
        "        global_pruning=False,\n",
        "        pruning_ratio=ratio,\n",
        "        iterative_steps=2,\n",
        "        ignored_layers=ignored_layers,\n",
        "    )\n",
        "\n",
        "    for _ in range(2):\n",
        "        pruner.step()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate)\n",
        "\n",
        "    EPOCHS = 5\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            labels = batch.pop(\"label\").to(device)\n",
        "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**inputs, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"📚 Epoch {epoch+1} — Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    acc = evaluate(model, val_dataset)\n",
        "    size_mb = measure_size(model)\n",
        "    infer_time = measure_inference_time(model, val_dataset)\n",
        "    macs, params = tp.utils.count_ops_and_params(wrapped_model, (example_inputs[\"input_ids\"], example_inputs[\"attention_mask\"]))\n",
        "\n",
        "    result = {\n",
        "        \"ratio\": ratio,\n",
        "        \"acc\": acc,\n",
        "        \"size_mb\": size_mb,\n",
        "        \"infer_time\": infer_time,\n",
        "        \"macs\": macs / 1e9,\n",
        "        \"params\": params / 1e6\n",
        "    }\n",
        "    results.append(result)\n",
        "    print(f\"✅ Done: {result}\")\n",
        "\n",
        "    del model, wrapped_model, pruner\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\n📊 All pruning results:\")\n",
        "for r in results:\n",
        "    print(f\"Ratio {r['ratio']:.1f} | Acc: {r['acc']:.4f} | Size: {r['size_mb']:.2f}MB | MACs: {r['macs']:.2f}G | Params: {r['params']:.2f}M | Inference: {r['infer_time']:.3f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "collapsed": true,
        "id": "0iSdIyD1vqir",
        "outputId": "9d2e94bf-062a-4b99-d082-303b95b672a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Testing pruning_ratio = 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-891846642.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"📚 Epoch {epoch+1} — Loss: {total_loss / len(train_loader):.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Quantisation (Optimum-'onnxruntime')"
      ],
      "metadata": {
        "id": "hHbS2c-C06No"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optimum[onnxruntime] onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "04okwafx82tS",
        "outputId": "618c53f6-bd73-4c89-cb8f-762515f71609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.18.0)\n",
            "Requirement already satisfied: optimum[onnxruntime] in /usr/local/lib/python3.11/dist-packages (1.27.0)\n",
            "Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (4.53.3)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (2.6.0+cu124)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (2.0.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (0.34.1)\n",
            "Requirement already satisfied: datasets>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (4.0.0)\n",
            "Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (5.29.5)\n",
            "Requirement already satisfied: onnxruntime>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (1.22.1)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (1.1.5)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.11.0->optimum[onnxruntime]) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (0.5.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (3.12.14)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[onnxruntime]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[onnxruntime]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[onnxruntime]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[onnxruntime]) (2025.7.14)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.11.0->optimum[onnxruntime]) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11->optimum[onnxruntime]) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.2.1->optimum[onnxruntime]) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.exporters.onnx import main_export\n",
        "\n",
        "main_export(\n",
        "    model_name_or_path='distilbert-base-uncased',\n",
        "    output=Path(\"onnx_model\"),\n",
        "    task=\"sequence-classification\",\n",
        "    opset=17,\n",
        "    library_name='transformers'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojQQo8RT87fm",
        "outputId": "8645cfe1-0a2c-45b0-cf4b-bfe5426c5c79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "quantize_dynamic(\n",
        "    model_input=\"onnx_model/model.onnx\",\n",
        "    model_output=\"onnx_model/model_quant.onnx\",\n",
        "    weight_type=QuantType.QInt8\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99m4PsO_-9Bz",
        "outputId": "0440b82d-5ca6-491a-8199-15cc7fa4e746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "session = ort.InferenceSession(\"onnx_model/model_quant.onnx\")\n",
        "\n",
        "inputs = tokenizer(\"This movie was great!\", return_tensors=\"np\", padding=\"max_length\", max_length=128)\n",
        "ort_inputs = {k: v for k, v in inputs.items()}\n",
        "outputs = session.run(None, ort_inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBlQIoYh_F-U",
        "outputId": "cd771ce5-12dc-4911-e7ae-f55f23a61bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[-0.1279223 ,  0.02694205]], dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Knowledge distillation"
      ],
      "metadata": {
        "id": "ZJEC-LXhAtIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/transformers-research-projects/\n",
        "%cd transformers-research-projects/distillation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgMfQsIRBPfb",
        "outputId": "09de53f7-2a78-4663-cd82-b70639c0626c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/transformers-research-projects/distillation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python distiller.py \\\n",
        "  --teacher_model ./finetuned_pruned_model \\\n",
        "  --student_model distilbert-base-uncased \\\n",
        "  --task_name sst2 \\\n",
        "  --output_dir /content/distil \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --learning_rate 5e-5 \\\n",
        "  --alpha_ce 0.5 \\\n",
        "  --alpha_hard 0.5 \\\n",
        "  --temperature 2.0 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --eval_steps 100 \\\n",
        "  --save_steps 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4UEyBGaBS1D",
        "outputId": "6690641d-aea9-4243-e395-ea8657d17079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-31 15:30:26.951328: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753975826.971068   27068 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753975826.977172   27068 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "07/31/2025 15:30:29 - INFO - numexpr.utils - PID: 27068 -  NumExpr defaulting to 2 threads.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Low-rank factorization"
      ],
      "metadata": {
        "id": "3TOM5arPELT0"
      }
    }
  ]
}